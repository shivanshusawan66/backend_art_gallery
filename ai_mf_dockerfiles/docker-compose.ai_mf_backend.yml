version: "3.3"

services:
  ai_mf_pipe:
    image: ai_mf_pipe:latest
    build:
      context: ../
      dockerfile: ./ai_mf_dockerfiles/Dockerfile.ai_mf_backend
    command: celery -A ai_mf_backend.celery_tasks worker --autoscale=${CELERY_AUTOSCALE_UPPER_LIMIT},${CELERY_AUTOSCALE_LOWER_LIMIT} --loglevel=${LOG_LEVEL} && celery -A ai_mf_backend.celery_tasks beat --loglevel=${LOG_LEVEL}
    container_name: ai_mf_pipe
    restart: always
    environment:
      LOG_LEVEL: ${LOG_LEVEL}
      ENVIRONMENT: server
    volumes:
      - ../ai_mf_backend/datafiles:/AIMFBackend/ai_mf_backend/datafiles
    depends_on:
      - ai_mf_tests
    networks:
      - ai_mf

  ai_mf_api:
    image: ai_mf_api:latest
    build:
      context: ../
      dockerfile: ./ai_mf_dockerfiles/Dockerfile.ai_mf_backend
    command: gunicorn -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000 --workers ${APPLICATION_WORKERS} --threads ${APPLICATION_THREADS} ai_mf_backend.api_application --timeout 400 --log-level=${LOG_LEVEL}
    container_name: ai_mf_api
    restart: always
    ports:
      - ${FASTAPI_APPLICATION_PORT}:8000
    environment:
      LOG_LEVEL: ${LOG_LEVEL}
      ENVIRONMENT: server
    volumes:
      - ../ai_mf_backend/datafiles:/AIMFBackend/ai_mf_backend/datafiles
    depends_on:
      - ai_mf_tests
    networks:
      - ai_mf

  # ai_mf_tests:
  #   image: ai_mf_api:latest
  #   build:
  #     context: ../
  #     dockerfile: ./ai_mf_dockerfiles/Dockerfile.ai_mf_backend
  #   command: pytest
  #   container_name: ai_mf_tests
  #   restart: no
  #   environment:
  #     LOG_LEVEL: ${LOG_LEVEL}
  #     ENVIRONMENT: server
  #   networks:
  #     - ai_mf

networks:
  ai_mf:
